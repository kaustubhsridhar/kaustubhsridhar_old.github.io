---
layout: default
title: Welcome To My Homepage
---

<div class="blurb">
	<font size="-1"><h1>Hi there, I'm Kaustubh Sridhar!</h1>
	<img src="/dp.png" alt="Kaustubh Sridhar" align="right" style="width:40%;" class="img_dp">
	<p>I'm a PhD candidate in <a href="https://www.ese.upenn.edu/">Electrical and Systems Enginnering</a> at the University of Pennsylvania, where I'm advised by <a href="https://www.cis.upenn.edu/~lee/home/index.shtml">Insup Lee</a>, <a href="https://www.seas.upenn.edu/~weimerj/research.html">James Weimer</a> and <a href="https://www.seas.upenn.edu/directory/profile.php?ID=91">Oleg Sokolsky</a></p>
		<p>I'm doing a summer internship at <a href="https://www.argo.ai/">Argo AI</a>'s autonomous vehicle security and functional safety (FuSa) team. Previously, I graduated with honors from the <a href="https://www.iitb.ac.in/">Indian Institute of Technology Bombay</a> and spent a summer as an intern at <a href="https://ece.duke.edu/">Duke University</a>.</p>
	
	<h4>Research</h4>
	<p>My research aims to build and analyze safety-critical learning-enabled cyber-physical systems (LE-CPS). Towards that goal, I'm working on, </p>

	  <!--			 We prove a sufficient condition for PoE of gradient descent is achieved when the learning rate is less than the inverse of the Lipschitz constant of the gradient of loss function. Our approach increases adversarial accuracy by up to 15% points on benchmark datasets and universally improves upon state-of-the-art adversarially trained models.
-->
	<h5><a href="https://arxiv.org/abs/2106.02078">Robust Learning via Persistency of Excitation</a></h5>
	<img src="/images/PoE.png" alt="PoE" align="left" style="width:30%;" class="img_rect">
	<p><a href="https://arxiv.org/abs/2106.02078">arXiv</a> / <a href="https://github.com/kaustubhsridhar/PoE-robustness">code</a> / <a href="https://robustbench.github.io/">leaderboard</a></p>
	<p>In adaptive control theory, maintaining persistency of excitation (PoE) is integral to ensuring convergence of parameter estimates in dynamical systems to their robust optima. In this work, we show that network training using gradient descent is equivalent to a dynamical system parameter estimation problem. Leveraging this relationship, we prove a sufficient condition for PoE of gradient descent (which has never been done before). In practice, ur appraoch leads to an universal increase the adversarial accuracy of standard and adversarially trained models.</p>

	<h4>Undergraduate Research</h4>

	<h4>Miscellaneous</h4>
	<p>I enjoy a game of tennis and squash and avidly read science-fiction.</p>
	
	<p>Some other links: </p>
	<ul class="contacts">
		<li><a href="https://linkedin.com/in/kaustubh-sridhar-8636797a/">LinkedIn</a></li>
		<li><a href="https://twitter.com/_k_sridhar">Twitter</a></li>
	</ul></font>
</div> 

